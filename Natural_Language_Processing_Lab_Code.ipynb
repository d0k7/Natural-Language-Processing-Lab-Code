{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNax9HapJjXakfHvTSuRvGR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d0k7/Natural-Language-Processing-Lab-Code/blob/main/Natural_Language_Processing_Lab_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Convert the text into tokens**"
      ],
      "metadata": {
        "id": "W4mAfRR7wtD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary library\n",
        "import nltk\n",
        "nltk.download('punkt')  # Download the punkt tokenizer\n",
        "\n",
        "# Function to convert text into tokens\n",
        "def text_to_tokens(text):\n",
        "    # Tokenize the text using the nltk word tokenizer\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Example text\n",
        "example_text = \"Convert this text into tokens using Python.\"\n",
        "\n",
        "# Call the function to get tokens\n",
        "result_tokens = text_to_tokens(example_text)\n",
        "\n",
        "# Print the original text\n",
        "print(\"Original Text:\")\n",
        "print(example_text)\n",
        "\n",
        "# Print the resulting tokens\n",
        "print(\"\\nTokens:\")\n",
        "print(result_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJCn_MXCw1DL",
        "outputId": "57e5ba2d-9671-4b2d-8a17-502d562a7045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "Convert this text into tokens using Python.\n",
            "\n",
            "Tokens:\n",
            "['Convert', 'this', 'text', 'into', 'tokens', 'using', 'Python', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Find the word frequency**"
      ],
      "metadata": {
        "id": "pEQsB0zXxpct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Function to find word frequency\n",
        "def word_frequency(tokens):\n",
        "    # Convert all words to lowercase for case-insensitive counting\n",
        "    lowercase_tokens = [word.lower() for word in tokens]\n",
        "\n",
        "    # Use Counter to count the occurrences of each word in the list of lowercase tokens\n",
        "    word_freq = Counter(lowercase_tokens)\n",
        "\n",
        "    return word_freq\n",
        "\n",
        "# Updated example tokens\n",
        "example_tokens = ['Convert', 'this', 'text', 'into', 'tokens', 'using', 'Python', 'python', '.']\n",
        "\n",
        "# Call the function to get word frequency\n",
        "result_word_freq = word_frequency(example_tokens)\n",
        "\n",
        "# Print the word frequency\n",
        "print(\"Word Frequency:\")\n",
        "print(result_word_freq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4qPHLxvyBHi",
        "outputId": "eb13c2f6-e694-4377-f118-84c51f5e2fbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Frequency:\n",
            "Counter({'python': 2, 'convert': 1, 'this': 1, 'text': 1, 'into': 1, 'tokens': 1, 'using': 1, '.': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Demonstrate a bigram language model**"
      ],
      "metadata": {
        "id": "PnYb3ztUyhjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import bigrams\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "# Function to create a bigram language model\n",
        "def create_bigram_model(tokens):\n",
        "    bigram_model = defaultdict(list)\n",
        "\n",
        "    # Generate bigrams from the list of tokens\n",
        "    bigrams_list = list(bigrams(tokens))\n",
        "\n",
        "    # Populate the bigram model with the next word for each current word\n",
        "    for prev_word, next_word in bigrams_list:\n",
        "        bigram_model[prev_word].append(next_word)\n",
        "\n",
        "    return bigram_model\n",
        "\n",
        "# Example tokens\n",
        "example_tokens = ['Convert', 'this', 'text', 'into', 'tokens', 'using', 'Python', 'python', '.']\n",
        "\n",
        "# Create a bigram language model\n",
        "bigram_model = create_bigram_model(example_tokens)\n",
        "\n",
        "# Function to generate text using the bigram model\n",
        "def generate_text(bigram_model, start_word, length=5):\n",
        "    generated_text = [start_word]\n",
        "\n",
        "    # Generate text based on the bigram model\n",
        "    for _ in range(length - 1):\n",
        "        current_word = generated_text[-1]\n",
        "\n",
        "        # Choose the next word randomly from the options in the bigram model\n",
        "        next_word_options = bigram_model[current_word]\n",
        "        if next_word_options:\n",
        "            next_word = random.choice(next_word_options)\n",
        "            generated_text.append(next_word)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Generate text starting with a given word\n",
        "start_word = 'Convert'\n",
        "generated_text = generate_text(bigram_model, start_word, length=8)\n",
        "\n",
        "# Print the generated text\n",
        "print(\"Generated Text:\")\n",
        "print(\" \".join(generated_text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dohwYXa0yus8",
        "outputId": "85e010cb-3f73-4286-fadf-5b31ffd6deba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "Convert this text into tokens using Python python\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Demonstrate a trigram language model**"
      ],
      "metadata": {
        "id": "ZN-uc70wy1Lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import trigrams\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "# Function to create a trigram language model\n",
        "def create_trigram_model(tokens):\n",
        "    trigram_model = defaultdict(list)\n",
        "\n",
        "    # Generate trigrams from the list of tokens\n",
        "    trigrams_list = list(trigrams(tokens))\n",
        "\n",
        "    # Populate the trigram model with the next word for each pair of current words\n",
        "    for word1, word2, next_word in trigrams_list:\n",
        "        trigram_model[(word1, word2)].append(next_word)\n",
        "\n",
        "    return trigram_model\n",
        "\n",
        "# Example tokens\n",
        "example_tokens = ['Convert', 'this', 'text', 'into', 'tokens', 'using', 'Python', 'python', '.']\n",
        "\n",
        "# Create a trigram language model\n",
        "trigram_model = create_trigram_model(example_tokens)\n",
        "\n",
        "# Function to generate text using the trigram model\n",
        "def generate_text(trigram_model, start_words, length=5):\n",
        "    generated_text = list(start_words)\n",
        "\n",
        "    # Generate text based on the trigram model\n",
        "    for _ in range(length - 2):\n",
        "        current_words = tuple(generated_text[-2:])\n",
        "\n",
        "        # Choose the next word randomly from the options in the trigram model\n",
        "        next_word_options = trigram_model[current_words]\n",
        "        if next_word_options:\n",
        "            next_word = random.choice(next_word_options)\n",
        "            generated_text.append(next_word)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Generate text starting with two given words\n",
        "start_words = ('Convert', 'this')\n",
        "generated_text = generate_text(trigram_model, start_words, length=8)\n",
        "\n",
        "# Print the generated text\n",
        "print(\"Generated Text:\")\n",
        "print(\" \".join(generated_text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHW6_n03zRSM",
        "outputId": "f5368b38-3948-4330-9445-215c1c126d85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "Convert this text into tokens using Python python\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Generate regular expression for a given text**"
      ],
      "metadata": {
        "id": "mJswAoA9zXxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Given text containing email addresses\n",
        "text_with_emails = \"Contact us at john.doe@example.com or jane.smith@email.com for more information.\"\n",
        "\n",
        "# Regular expression for matching email addresses\n",
        "email_regex = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "\n",
        "# Find all matches in the text\n",
        "matches = re.findall(email_regex, text_with_emails)\n",
        "\n",
        "# Print the matches\n",
        "print(\"Email Addresses:\")\n",
        "print(matches)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAM_cUzXzR1h",
        "outputId": "5518a2f1-9950-4d72-8ebf-45892c09cf5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Email Addresses:\n",
            "['john.doe@example.com', 'jane.smith@email.com']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Perform Lemmatization**"
      ],
      "metadata": {
        "id": "uL6TFQiDzpen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')  # Download the punkt tokenizer\n",
        "nltk.download('wordnet')  # Download the WordNet database\n",
        "\n",
        "# Function to perform lemmatization on a list of tokens\n",
        "def lemmatize_tokens(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Example text\n",
        "example_text = \"Convert this text into tokens and perform lemmatization on each token.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(example_text)\n",
        "\n",
        "# Perform lemmatization on the tokens\n",
        "lemmatized_tokens = lemmatize_tokens(tokens)\n",
        "\n",
        "# Print the original tokens and the lemmatized tokens\n",
        "print(\"Original Tokens:\")\n",
        "print(tokens)\n",
        "print(\"\\nLemmatized Tokens:\")\n",
        "print(lemmatized_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQlmr0FGzj03",
        "outputId": "1ef39d17-dbbb-4b5a-bb75-18586bd117fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens:\n",
            "['Convert', 'this', 'text', 'into', 'tokens', 'and', 'perform', 'lemmatization', 'on', 'each', 'token', '.']\n",
            "\n",
            "Lemmatized Tokens:\n",
            "['Convert', 'this', 'text', 'into', 'token', 'and', 'perform', 'lemmatization', 'on', 'each', 'token', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Perform Stemming**"
      ],
      "metadata": {
        "id": "DG8pOi9Mz5LX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')  # Download the punkt tokenizer\n",
        "\n",
        "# Function to perform stemming on a list of tokens\n",
        "def stem_tokens(tokens):\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    return stemmed_tokens\n",
        "\n",
        "# Example text\n",
        "example_text = \"Convert this text into tokens and perform stemming on each token.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(example_text)\n",
        "\n",
        "# Perform stemming on the tokens\n",
        "stemmed_tokens = stem_tokens(tokens)\n",
        "\n",
        "# Print the original tokens and the stemmed tokens\n",
        "print(\"Original Tokens:\")\n",
        "print(tokens)\n",
        "print(\"\\nStemmed Tokens:\")\n",
        "print(stemmed_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKIclWGLz0FB",
        "outputId": "49539599-93d3-4e77-83f4-83892362e31f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens:\n",
            "['Convert', 'this', 'text', 'into', 'tokens', 'and', 'perform', 'stemming', 'on', 'each', 'token', '.']\n",
            "\n",
            "Stemmed Tokens:\n",
            "['convert', 'thi', 'text', 'into', 'token', 'and', 'perform', 'stem', 'on', 'each', 'token', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Identify parts-of Speech using Penn Treebank tag set.**"
      ],
      "metadata": {
        "id": "a4cFKdgF0E7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')  # Download the punkt tokenizer\n",
        "nltk.download('averaged_perceptron_tagger')  # Download the tagger model\n",
        "\n",
        "# Example text\n",
        "example_text = \"Identify parts of speech using the Penn Treebank tag set.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(example_text)\n",
        "\n",
        "# Perform part-of-speech tagging using the Penn Treebank tag set\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "# Print the original tokens and their part-of-speech tags\n",
        "print(\"Original Tokens:\")\n",
        "print(tokens)\n",
        "print(\"\\nPart-of-Speech Tags (Penn Treebank):\")\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlDpovrh0Cu6",
        "outputId": "f7aa2e75-8800-49be-8c13-eb40bcdea8df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens:\n",
            "['Identify', 'parts', 'of', 'speech', 'using', 'the', 'Penn', 'Treebank', 'tag', 'set', '.']\n",
            "\n",
            "Part-of-Speech Tags (Penn Treebank):\n",
            "[('Identify', 'NNP'), ('parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('using', 'VBG'), ('the', 'DT'), ('Penn', 'NNP'), ('Treebank', 'NNP'), ('tag', 'NN'), ('set', 'NN'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Implement HMM for POS tagging**"
      ],
      "metadata": {
        "id": "rnaWQz_60PmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from nltk.tag import hmm\n",
        "\n",
        "# Download the Brown corpus\n",
        "nltk.download('brown')\n",
        "\n",
        "# Get the tagged sentences from the Brown corpus\n",
        "tagged_sentences = brown.tagged_sents(categories='news')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_size = int(0.8 * len(tagged_sentences))\n",
        "train_data, test_data = tagged_sentences[:train_size], tagged_sentences[train_size:]\n",
        "\n",
        "# Train the HMM POS tagger\n",
        "hmm_tagger = hmm.HiddenMarkovModelTrainer().train(train_data)\n",
        "\n",
        "# Evaluate the performance on the test set\n",
        "accuracy = hmm_tagger.evaluate(test_data)\n",
        "print(f\"HMM POS Tagger Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "# Example of tagging a new sequence\n",
        "new_sequence = \"Implement a Hidden Markov Model for Part-of-Speech tagging.\"\n",
        "tokenized_sequence = nltk.word_tokenize(new_sequence)\n",
        "predicted_tags = hmm_tagger.tag(tokenized_sequence)\n",
        "\n",
        "# Print the predicted POS tags for the new sequence\n",
        "print(\"\\nPredicted POS Tags:\")\n",
        "print(predicted_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW4aQ7aA0NUP",
        "outputId": "24bce033-44e8-4045-8fb8-17c24f1aa9cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "<ipython-input-11-6dfa56c9051a>:19: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  accuracy = hmm_tagger.evaluate(test_data)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HMM POS Tagger Accuracy: 28.96%\n",
            "\n",
            "Predicted POS Tags:\n",
            "[('Implement', 'AT'), ('a', 'AT'), ('Hidden', 'AT'), ('Markov', 'AT'), ('Model', 'AT'), ('for', 'AT'), ('Part-of-Speech', 'AT'), ('tagging', 'AT'), ('.', 'AT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Build a Chunker**"
      ],
      "metadata": {
        "id": "GZWBovtI25em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import treebank_chunk\n",
        "\n",
        "# Download the 'treebank' corpus\n",
        "nltk.download('treebank')\n",
        "\n",
        "# Get the training data from the Treebank chunk corpus\n",
        "train_data = treebank_chunk.chunked_sents()\n",
        "\n",
        "# Define a simple NP chunking grammar using regular expressions\n",
        "chunking_grammar = r\"\"\"\n",
        "    NP: {<DT|JJ|NN.*>+}          # Chunk sequences of determiner, adjective, and noun\n",
        "\"\"\"\n",
        "\n",
        "# Create a chunk parser using the defined grammar\n",
        "chunk_parser = nltk.RegexpParser(chunking_grammar)\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "# POS tagging\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "# Perform chunking\n",
        "tree = chunk_parser.parse(pos_tags)\n",
        "\n",
        "# Print the resulting tree in a text-based format\n",
        "print(\"Chunked Tree:\")\n",
        "print(tree)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfZCPymN0XzE",
        "outputId": "94d7463d-5952-4659-fa67-58a72b311a85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunked Tree:\n",
            "(S\n",
            "  (NP The/DT quick/JJ brown/NN fox/NN)\n",
            "  jumps/VBZ\n",
            "  over/IN\n",
            "  (NP the/DT lazy/JJ dog/NN)\n",
            "  ./.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. Find the synonym of a word using WordNet**"
      ],
      "metadata": {
        "id": "EuUurE0P3bYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Function to get synonyms of a word using WordNet\n",
        "def get_synonyms(word):\n",
        "    synonyms = []\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.append(lemma.name())\n",
        "    return set(synonyms)\n",
        "\n",
        "# Example word\n",
        "word = \"happy\"\n",
        "\n",
        "# Get synonyms of the word\n",
        "synonyms = get_synonyms(word)\n",
        "\n",
        "# Print the synonyms\n",
        "print(f\"Synonyms of '{word}':\")\n",
        "print(synonyms)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaXL-9Ve3E_Z",
        "outputId": "daeef515-1405-4ab6-ed27-e5425033bee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synonyms of 'happy':\n",
            "{'happy', 'well-chosen', 'glad', 'felicitous'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12. Find the antonym of a word**"
      ],
      "metadata": {
        "id": "_tgzUG_N3rVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Function to get antonyms of a word using WordNet\n",
        "def get_antonyms(word):\n",
        "    antonyms = []\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            if lemma.antonyms():\n",
        "                antonyms.append(lemma.antonyms()[0].name())\n",
        "    return set(antonyms)\n",
        "\n",
        "# Example word\n",
        "word = \"happy\"\n",
        "\n",
        "# Get antonyms of the word\n",
        "antonyms = get_antonyms(word)\n",
        "\n",
        "# Print the antonyms\n",
        "print(f\"Antonyms of '{word}':\")\n",
        "print(antonyms)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTzqecG63pxa",
        "outputId": "c3c55a6b-778c-4bde-84d7-413539230d83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Antonyms of 'happy':\n",
            "{'unhappy'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13. Implement semantic role labeling to identify named entities**"
      ],
      "metadata": {
        "id": "IQ_ceHXm31w6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -U spacy\n",
        "#!python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load the English spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"Apple is considering opening a new store in Mumbai.\"\n",
        "\n",
        "# Process the sentence with spaCy\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Print named entities\n",
        "print(\"Named Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text}: {ent.label_}\")\n",
        "\n",
        "# Print semantic roles\n",
        "print(\"\\nSemantic Roles:\")\n",
        "for token in doc:\n",
        "    print(f\"{token.text}: {token.dep_} -> {token.head.text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdvBo_nM3z7W",
        "outputId": "905d6faa-396d-4aea-aedc-4f59427a0fc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities:\n",
            "Apple: ORG\n",
            "Mumbai: GPE\n",
            "\n",
            "Semantic Roles:\n",
            "Apple: nsubj -> considering\n",
            "is: aux -> considering\n",
            "considering: ROOT -> considering\n",
            "opening: xcomp -> considering\n",
            "a: det -> store\n",
            "new: amod -> store\n",
            "store: dobj -> opening\n",
            "in: prep -> opening\n",
            "Mumbai: pobj -> in\n",
            ".: punct -> considering\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**14. Resolve the ambiguity**"
      ],
      "metadata": {
        "id": "d_sbJ68Z4gbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.wsd import lesk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the WordNet data\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Example sentence with ambiguity\n",
        "sentence = \"I went to the bank to deposit some money.\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "# Perform word sense disambiguation using Lesk algorithm\n",
        "ambiguous_word = \"bank\"\n",
        "synset = lesk(tokens, ambiguous_word)\n",
        "\n",
        "# Print the sense and definition\n",
        "if synset:\n",
        "    print(f\"Ambiguous Word: {ambiguous_word}\")\n",
        "    print(f\"Selected Sense: {synset.name()}\")\n",
        "    print(f\"Definition: {synset.definition()}\")\n",
        "else:\n",
        "    print(f\"No suitable sense found for the word '{ambiguous_word}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-Xm7q1e4E7c",
        "outputId": "4da9b6de-3792-4b45-8611-dcfcf191b851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ambiguous Word: bank\n",
            "Selected Sense: savings_bank.n.02\n",
            "Definition: a container (usually with a slot in the top) for keeping money at home\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**15. Implement RNN for sequence labeling**"
      ],
      "metadata": {
        "id": "3-lydjJx4xT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow numpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPJVwwhV4qoG",
        "outputId": "5e1bbe32-ffbc-4c90-df8c-f8f89477999d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "\n",
        "# Generate dummy data\n",
        "# Each sentence is represented as a sequence of word indices, and each label is represented as a sequence of label indices\n",
        "sentences = [\n",
        "    [1, 2, 3, 4, 5],\n",
        "    [6, 7, 8, 9],\n",
        "    [10, 11, 12, 13, 14, 15],\n",
        "]\n",
        "\n",
        "labels = [\n",
        "    [0, 1, 0, 1, 2],  # 0: O, 1: B-entity, 2: I-entity\n",
        "    [0, 1, 0, 1],\n",
        "    [0, 1, 2, 0, 1, 0],\n",
        "]\n",
        "\n",
        "# Pad sequences to have the same length\n",
        "max_len = max(len(seq) for seq in sentences)\n",
        "sentences_padded = tf.keras.preprocessing.sequence.pad_sequences(sentences, padding='post', maxlen=max_len)\n",
        "labels_padded = tf.keras.preprocessing.sequence.pad_sequences(labels, padding='post', maxlen=max_len)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=16, output_dim=8, input_length=max_len))  # Adjust input_dim based on your vocabulary size\n",
        "model.add(SimpleRNN(16, return_sequences=True))\n",
        "model.add(Dense(3, activation='softmax'))  # Assuming 3 classes (O, B-entity, I-entity)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(sentences_padded, labels_padded, epochs=10, batch_size=1)\n",
        "\n",
        "# Make predictions (you would typically use a validation set or test set)\n",
        "predictions = model.predict(sentences_padded)\n",
        "\n",
        "# Print predictions\n",
        "for i in range(len(sentences)):\n",
        "    print(f\"Sentence: {sentences[i]}\")\n",
        "    print(f\"Predicted Labels: {np.argmax(predictions[i], axis=-1)}\")\n",
        "    print(f\"True Labels: {labels[i]}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bk_iSIAC4-Je",
        "outputId": "5a931741-cb53-435a-e720-75ef0181cc30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3/3 [==============================] - 2s 10ms/step - loss: 1.0708 - accuracy: 0.6667\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.0579 - accuracy: 0.7222\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 1.0428 - accuracy: 0.6667\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 1.0288 - accuracy: 0.6667\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 1.0159 - accuracy: 0.6111\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 1.0019 - accuracy: 0.5556\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.9859 - accuracy: 0.5556\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.9726 - accuracy: 0.5556\n",
            "Epoch 9/10\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.9585 - accuracy: 0.5556\n",
            "Epoch 10/10\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.9435 - accuracy: 0.5556\n",
            "1/1 [==============================] - 0s 250ms/step\n",
            "Sentence: [1, 2, 3, 4, 5]\n",
            "Predicted Labels: [0 0 0 0 0 0]\n",
            "True Labels: [0, 1, 0, 1, 2]\n",
            "\n",
            "Sentence: [6, 7, 8, 9]\n",
            "Predicted Labels: [0 0 0 0 0 0]\n",
            "True Labels: [0, 1, 0, 1]\n",
            "\n",
            "Sentence: [10, 11, 12, 13, 14, 15]\n",
            "Predicted Labels: [0 0 0 0 0 0]\n",
            "True Labels: [0, 1, 2, 0, 1, 0]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**16. Implement POS tagging using LSTM**"
      ],
      "metadata": {
        "id": "L9JTHLrJw25Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dummy data\n",
        "sentences = [\n",
        "    \"The cat sat on the mat\",\n",
        "    \"I love natural language processing\",\n",
        "    \"POS tagging with LSTM is interesting\",\n",
        "]\n",
        "\n",
        "pos_tags = [\n",
        "    [\"DT\", \"NN\", \"VBD\", \"IN\", \"DT\", \"NN\"],\n",
        "    [\"PRP\", \"VBP\", \"JJ\", \"NN\", \"NN\", \"VBG\"],\n",
        "    [\"NN\", \"VBG\", \"IN\", \"NN\", \"VBZ\", \"JJ\"],\n",
        "]\n",
        "\n",
        "# Tokenize words and POS tags\n",
        "tokenizer_words = Tokenizer(oov_token=\"<OOV>\")\n",
        "tokenizer_pos = Tokenizer()\n",
        "\n",
        "tokenizer_words.fit_on_texts(sentences)\n",
        "tokenizer_pos.fit_on_texts(pos_tags)\n",
        "\n",
        "vocab_size_words = len(tokenizer_words.word_index) + 1\n",
        "vocab_size_pos = len(tokenizer_pos.word_index) + 1\n",
        "\n",
        "# Convert sentences and POS tags to sequences\n",
        "sequences_words = tokenizer_words.texts_to_sequences(sentences)\n",
        "sequences_pos = tokenizer_pos.texts_to_sequences(pos_tags)\n",
        "\n",
        "# Pad sequences to have the same length\n",
        "max_len = max(len(seq) for seq in sequences_words)\n",
        "padded_sequences_words = pad_sequences(sequences_words, padding='post', maxlen=max_len)\n",
        "padded_sequences_pos = pad_sequences(sequences_pos, padding='post', maxlen=max_len)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences_words, padded_sequences_pos, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size_words, output_dim=8, input_length=max_len, mask_zero=True))\n",
        "model.add(LSTM(16, return_sequences=True))\n",
        "model.add(Dense(vocab_size_pos, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=1, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Make predictions on new sentences\n",
        "new_sentences = [\"The dog barked loudly\", \"I enjoy reading books\"]\n",
        "new_sequences = tokenizer_words.texts_to_sequences(new_sentences)\n",
        "new_padded_sequences = pad_sequences(new_sequences, padding='post', maxlen=max_len)\n",
        "predictions = model.predict(new_padded_sequences)\n",
        "\n",
        "# Print predictions\n",
        "for i, sentence in enumerate(new_sentences):\n",
        "    predicted_pos_tags = [tokenizer_pos.index_word[np.argmax(pred)] for pred in predictions[i]]\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Predicted POS Tags: {predicted_pos_tags}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zyx8qw4kw6PG",
        "outputId": "2a2e2798-574f-4070-db19-dab27e8592b8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2/2 [==============================] - 7s 2s/step - loss: 2.3026 - accuracy: 0.2727 - val_loss: 2.3024 - val_accuracy: 0.1667\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 2.2990 - accuracy: 0.1818 - val_loss: 2.3022 - val_accuracy: 0.3333\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 0s 178ms/step - loss: 2.2957 - accuracy: 0.4545 - val_loss: 2.3021 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 0s 145ms/step - loss: 2.2924 - accuracy: 0.3636 - val_loss: 2.3020 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 0s 200ms/step - loss: 2.2891 - accuracy: 0.3636 - val_loss: 2.3019 - val_accuracy: 0.3333\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 0s 145ms/step - loss: 2.2858 - accuracy: 0.3636 - val_loss: 2.3017 - val_accuracy: 0.3333\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 2.2820 - accuracy: 0.3636 - val_loss: 2.3016 - val_accuracy: 0.3333\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 0s 89ms/step - loss: 2.2785 - accuracy: 0.3636 - val_loss: 2.3015 - val_accuracy: 0.3333\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 0s 147ms/step - loss: 2.2755 - accuracy: 0.3636 - val_loss: 2.3014 - val_accuracy: 0.3333\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 0s 249ms/step - loss: 2.2712 - accuracy: 0.3636 - val_loss: 2.3012 - val_accuracy: 0.3333\n",
            "1/1 [==============================] - 5s 5s/step - loss: 2.3012 - accuracy: 0.3333\n",
            "Test Loss: 2.3012\n",
            "Test Accuracy: 0.3333\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "Sentence: The dog barked loudly\n",
            "Predicted POS Tags: ['nn', 'nn', 'nn', 'nn', 'nn', 'nn']\n",
            "\n",
            "Sentence: I enjoy reading books\n",
            "Predicted POS Tags: ['nn', 'nn', 'nn', 'nn', 'nn', 'nn']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**17. Implement Named Entity Recognizer**"
      ],
      "metadata": {
        "id": "iti1e02cxKm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, TimeDistributed\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dummy data\n",
        "sentences = [\n",
        "    \"Apple is a company based in Cupertino.\",\n",
        "    \"John works at Google.\",\n",
        "    \"Paris is the capital of France.\",\n",
        "]\n",
        "\n",
        "ner_labels = [\n",
        "    [\"B-org\", \"O\", \"O\", \"O\", \"B-geo\", \"O\"],\n",
        "    [\"B-per\", \"O\", \"O\", \"O\", \"B-org\", \"O\"],\n",
        "    [\"B-geo\", \"O\", \"O\", \"O\", \"B-geo\", \"O\"],\n",
        "]\n",
        "\n",
        "# Tokenize words and NER labels\n",
        "tokenizer_words = Tokenizer(oov_token=\"<OOV>\")\n",
        "tokenizer_labels = Tokenizer()\n",
        "\n",
        "tokenizer_words.fit_on_texts(sentences)\n",
        "tokenizer_labels.fit_on_texts(ner_labels)\n",
        "\n",
        "vocab_size_words = len(tokenizer_words.word_index) + 1\n",
        "vocab_size_labels = len(tokenizer_labels.word_index) + 1\n",
        "\n",
        "# Convert sentences and NER labels to sequences\n",
        "sequences_words = tokenizer_words.texts_to_sequences(sentences)\n",
        "sequences_labels = tokenizer_labels.texts_to_sequences(ner_labels)\n",
        "\n",
        "# Pad sequences to have the same length\n",
        "max_len = max(len(seq) for seq in sequences_words)\n",
        "padded_sequences_words = pad_sequences(sequences_words, padding='post', maxlen=max_len)\n",
        "padded_sequences_labels = pad_sequences(sequences_labels, padding='post', maxlen=max_len)\n",
        "\n",
        "# Convert NER labels to categorical format\n",
        "categorical_labels = tf.keras.utils.to_categorical(padded_sequences_labels, num_classes=vocab_size_labels)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences_words, categorical_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size_words, output_dim=8, input_length=max_len, mask_zero=True))\n",
        "model.add(Bidirectional(LSTM(16, return_sequences=True)))\n",
        "model.add(TimeDistributed(Dense(vocab_size_labels, activation='softmax')))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=1, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Make predictions on new sentences\n",
        "new_sentences = [\"Microsoft is located in Redmond.\", \"Anna lives in Berlin.\"]\n",
        "new_sequences = tokenizer_words.texts_to_sequences(new_sentences)\n",
        "new_padded_sequences = pad_sequences(new_sequences, padding='post', maxlen=max_len)\n",
        "predictions = model.predict(new_padded_sequences)\n",
        "\n",
        "# Print predictions\n",
        "for i, sentence in enumerate(new_sentences):\n",
        "    predicted_ner_labels = [tokenizer_labels.index_word[np.argmax(pred)] for pred in predictions[i]]\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Predicted NER Labels: {predicted_ner_labels}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "intM-4ZVw87i",
        "outputId": "0b5ee1b6-f3cd-42d3-bef4-a6eef9cfcb9d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2/2 [==============================] - 12s 2s/step - loss: 1.6114 - accuracy: 0.0000e+00 - val_loss: 1.6042 - val_accuracy: 0.5714\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 1.6051 - accuracy: 0.6000 - val_loss: 1.6005 - val_accuracy: 0.5714\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 1.5991 - accuracy: 0.7000 - val_loss: 1.5966 - val_accuracy: 0.5714\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 1.5930 - accuracy: 0.7000 - val_loss: 1.5927 - val_accuracy: 0.5714\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 1.5867 - accuracy: 0.7000 - val_loss: 1.5888 - val_accuracy: 0.5714\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 1.5804 - accuracy: 0.7000 - val_loss: 1.5848 - val_accuracy: 0.5714\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 1.5740 - accuracy: 0.7000 - val_loss: 1.5805 - val_accuracy: 0.5714\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 1.5672 - accuracy: 0.7000 - val_loss: 1.5762 - val_accuracy: 0.5714\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 1.5603 - accuracy: 0.7000 - val_loss: 1.5716 - val_accuracy: 0.5714\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 1.5530 - accuracy: 0.7000 - val_loss: 1.5668 - val_accuracy: 0.5714\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.5668 - accuracy: 0.5714\n",
            "Test Loss: 1.5668\n",
            "Test Accuracy: 0.5714\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "Sentence: Microsoft is located in Redmond.\n",
            "Predicted NER Labels: ['o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
            "\n",
            "Sentence: Anna lives in Berlin.\n",
            "Predicted NER Labels: ['o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**18. Word sense disambiguation by LSTM/GRU**"
      ],
      "metadata": {
        "id": "OWdA_NEmxSb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dummy data\n",
        "sentences = [\n",
        "    \"I went to the bank to deposit some money.\",\n",
        "    \"The river bank was full of fish.\",\n",
        "    \"He sat on the bank of the river and enjoyed the view.\",\n",
        "]\n",
        "\n",
        "word_senses = [1, 2, 3]  # Assign unique labels for each sense\n",
        "\n",
        "# Tokenize words and word senses\n",
        "tokenizer_words = Tokenizer(oov_token=\"<OOV>\")\n",
        "tokenizer_senses = Tokenizer()\n",
        "\n",
        "tokenizer_words.fit_on_texts(sentences)\n",
        "tokenizer_senses.fit_on_texts(map(str, word_senses))\n",
        "\n",
        "vocab_size_words = len(tokenizer_words.word_index) + 1\n",
        "vocab_size_senses = len(tokenizer_senses.word_index) + 1\n",
        "\n",
        "# Convert sentences and word senses to sequences\n",
        "sequences_words = tokenizer_words.texts_to_sequences(sentences)\n",
        "sequences_senses = tokenizer_senses.texts_to_sequences(map(str, word_senses))\n",
        "\n",
        "# Pad sequences to have the same length\n",
        "max_len = max(len(seq) for seq in sequences_words)\n",
        "padded_sequences_words = pad_sequences(sequences_words, padding='post', maxlen=max_len)\n",
        "padded_sequences_senses = pad_sequences(sequences_senses, padding='post', maxlen=max_len)\n",
        "\n",
        "# Convert word senses to categorical format\n",
        "categorical_senses = tf.keras.utils.to_categorical(padded_sequences_senses, num_classes=vocab_size_senses)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences_words, categorical_senses, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size_words, output_dim=8, input_length=max_len, mask_zero=True))\n",
        "model.add(LSTM(16, return_sequences=True))\n",
        "model.add(Dense(vocab_size_senses, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=1, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Make predictions on new sentences\n",
        "new_sentences = [\"I saw a bird at the river bank.\", \"He invested money in the bank.\"]\n",
        "new_sequences = tokenizer_words.texts_to_sequences(new_sentences)\n",
        "new_padded_sequences = pad_sequences(new_sequences, padding='post', maxlen=max_len)\n",
        "predictions = model.predict(new_padded_sequences)\n",
        "\n",
        "# Print predictions\n",
        "for i, sentence in enumerate(new_sentences):\n",
        "    predicted_word_senses = [tokenizer_senses.word_index.get(np.argmax(pred), 'Unknown') for pred in predictions[i]]\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Predicted Word Senses: {predicted_word_senses}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-JzeSDwxXz0",
        "outputId": "fa201a50-4905-45d2-f610-eb0e729c97f6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2/2 [==============================] - 5s 1s/step - loss: 1.3836 - accuracy: 0.7895 - val_loss: 1.3750 - val_accuracy: 0.8889\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 1.3757 - accuracy: 0.8947 - val_loss: 1.3674 - val_accuracy: 0.8889\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 1.3672 - accuracy: 0.8947 - val_loss: 1.3594 - val_accuracy: 0.8889\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 1.3585 - accuracy: 0.8947 - val_loss: 1.3512 - val_accuracy: 0.8889\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 1.3495 - accuracy: 0.8947 - val_loss: 1.3425 - val_accuracy: 0.8889\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 1.3395 - accuracy: 0.8947 - val_loss: 1.3335 - val_accuracy: 0.8889\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 1.3295 - accuracy: 0.8947 - val_loss: 1.3240 - val_accuracy: 0.8889\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 1.3195 - accuracy: 0.8947 - val_loss: 1.3138 - val_accuracy: 0.8889\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 1.3081 - accuracy: 0.8947 - val_loss: 1.3031 - val_accuracy: 0.8889\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 1.2959 - accuracy: 0.8947 - val_loss: 1.2918 - val_accuracy: 0.8889\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.2918 - accuracy: 0.8889\n",
            "Test Loss: 1.2918\n",
            "Test Accuracy: 0.8889\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "Sentence: I saw a bird at the river bank.\n",
            "Predicted Word Senses: ['Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown']\n",
            "\n",
            "Sentence: He invested money in the bank.\n",
            "Predicted Word Senses: ['Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**19. Develop a Movie review system**"
      ],
      "metadata": {
        "id": "e3jvmEeeyARC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Dummy data (replace this with your own dataset)\n",
        "reviews = [\n",
        "    \"The movie was fantastic! I loved every moment of it.\",\n",
        "    \"Terrible acting and a boring plot. I regret watching it.\",\n",
        "    \"A masterpiece! The performances were outstanding.\",\n",
        "    \"The film was okay, but nothing special.\",\n",
        "]\n",
        "\n",
        "labels = [1, 0, 1, 0]  # 1 for positive, 0 for negative\n",
        "\n",
        "# Tokenize words\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(reviews)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Convert sentences to sequences\n",
        "sequences = tokenizer.texts_to_sequences(reviews)\n",
        "\n",
        "# Pad sequences to have the same length\n",
        "max_len = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, padding='post', maxlen=max_len)\n",
        "\n",
        "# Convert labels to numpy array\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=8, input_length=max_len))\n",
        "model.add(LSTM(16))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=1, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Make predictions on new reviews\n",
        "new_reviews = [\"The best movie I've ever seen!\", \"Awful waste of time.\"]\n",
        "new_sequences = tokenizer.texts_to_sequences(new_reviews)\n",
        "new_padded_sequences = pad_sequences(new_sequences, padding='post', maxlen=max_len)\n",
        "predictions = model.predict(new_padded_sequences)\n",
        "\n",
        "# Print predictions\n",
        "for i, review in enumerate(new_reviews):\n",
        "    sentiment = \"Positive\" if predictions[i] > 0.5 else \"Negative\"\n",
        "    print(f\"Review: {review}\")\n",
        "    print(f\"Predicted Sentiment: {sentiment}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6rRTdG9xoD5",
        "outputId": "72dbfb89-f359-4764-a05a-45306aae74ce"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "3/3 [==============================] - 4s 467ms/step - loss: 0.6922 - accuracy: 0.6667 - val_loss: 0.6966 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/5\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.6892 - accuracy: 0.6667 - val_loss: 0.7002 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/5\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.6865 - accuracy: 0.6667 - val_loss: 0.7027 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/5\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.6849 - accuracy: 0.6667 - val_loss: 0.7048 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/5\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.6831 - accuracy: 0.6667 - val_loss: 0.7074 - val_accuracy: 0.0000e+00\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.7074 - accuracy: 0.0000e+00\n",
            "Test Loss: 0.7074\n",
            "Test Accuracy: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7995602780d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 476ms/step\n",
            "Review: The best movie I've ever seen!\n",
            "Predicted Sentiment: Positive\n",
            "\n",
            "Review: Awful waste of time.\n",
            "Predicted Sentiment: Positive\n",
            "\n"
          ]
        }
      ]
    }
  ]
}